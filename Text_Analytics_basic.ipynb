{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic\n",
    "\n",
    "### 자연어 처리 (National Language Processing)\n",
    "   * 머신이 인간의 언어를 이해하고 해석하는 데 중점을 둔 기술 \n",
    "   * 언어를 해석하기 위한 기계 번역, 자동으로 질문을 해석하고 답을 해주는 질의 응답 시스템 등을 주로 수행  \n",
    "   \n",
    "### 텍스트 분석 (Text Analytics) = 텍스트 마이닝 (Text Mining)\n",
    "   * 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점을 둔 기술\n",
    "   * 머신러닝, 언어 이해, 통계 등을 활용해 모델을 수립하고 정보를 추출해 비즈니스 인텔리전스나 예측 분석 등의 분석 작업을 주로 수행 \n",
    "   \n",
    "머신러닝이 보편화 되면서 두 영역 사이의 구분이 크게 의미는 없음.\n",
    "\n",
    "### 텍스트 분석 기술 종류\n",
    "* 텍스트 분석 (Text Classification) \n",
    "> 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법 ( 지도 학습 적용 ) <br><br>\n",
    "ex) 특정 신문 기사 내용이 연애 / 정치 / 사회 / 문화 중 어떤 카테고리에 속하는지 자동으로 분류하거나 스팸 메임 검출 같은 프로그램 등 \n",
    "\n",
    "\n",
    "* 감성 분석 (Sentiment Analysis)\n",
    "> 텍스트에서 나타나는 감정 / 판단 / 믿음 / 의견 / 기분 등의 주관적인 요소를 분석하는 기법 ( 지도 , 비지도 학습 적용 ) <br><br>\n",
    "ex) 소셜 미디어 감정 분석, 영화나 제품 리뷰에 대한 긍정 부정, 여론 조사 의견 분석 등 \n",
    "\n",
    "* 텍스트 요약 (Summarization)\n",
    "> 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법 <br><br>\n",
    "ex) Topic modeling \n",
    "\n",
    "* 텍스트 군집화와 유사도 측정 (Clustering) \n",
    "> 비슷한 유형의 문서에 대해 군집화를 수행하거나 문서들간의 유사도를 측정해 비슷한 문서끼리 모으는 기법 ( 비지도 학습 적용 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 이해 \n",
    "\n",
    "지금까지는 주어진 정형 데이터 기반에서 ML 모델을 학습 시키고 예측을 수행한 반면, 텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것이다.<br> 여기서 텍스트는 신문기사, 책과 같은 article 의 의미로 받아 들이면 된다. [비교](https://image.slidesharecdn.com/textminingbasic181121-181121132532/95/tf-idf-4-638.jpg?cb=1542806769)\n",
    "* 정형 데이터 \n",
    "    * 데이터베이스의 테이블에 저장 될 수 있는 데이터의 형태 \n",
    "    * ex) 숫자형 , 카테고리형 등<br><br>\n",
    "* 비정형 데이터 \n",
    "    * 특정한 형태가 없으며 연산도 불가능한 데이터 \n",
    "    * ex) 텍스트, 영상 , 이미지 , 음성 등 <br>\n",
    "\n",
    "그렇다면 비정형 데이터에 ML 알고리즘을 적용하기 위해서 어떻게 피처 형태로 추출하며, 추출된 피처에 의미 있는 값을 부여 할 수 있을까?<br><br>\n",
    "텍스트를 word 로 기반한 다수의 피처로 추출하고 이 피처에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 단어의 조합인 벡터값으로 표현 될 수 있는데, <br>이렇게 텍스트를 변환하는 것을 <span style=\"color:red\">피처 벡터화 / 피처 추출</span> 이라고 한다. [EX](https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_637/http://hleecaster.com/wp-content/uploads/2020/01/bow01.jpg)<br><br>\n",
    "대표적인 피처 벡터화 방법에는 BOW (Bag of Words) 와 Word2Vec 방법이 있다.  <br><br>\n",
    "__텍스트 분석 수행 프로세스__ 를 정리해 보자면, ( p.468 참고 ) \n",
    "1. <u>텍스트 전처리</u> \n",
    "    * 텍스트를 피처로 만들기 전에 미리 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, word 등의 토큰화 작업, 의미 없는 단어 (Stop word)   제거 작업,<br> 어근 추출 (Stemming / Lemmatization ) 등의 텍스트 정규화 작업을 수행 \n",
    "    \n",
    "\n",
    "2. <u>피처 벡터화 /  피처 추출</u>\n",
    "    * 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당. <br> BOW , Word2Vec 이 대표적이며 BOW 는 Count 기반과 TF-IDF 기반 벡터화가 있음. \n",
    "    \n",
    "\n",
    "3. <u>ML 모델 수립 및 학습 / 예측 / 평가</u> \n",
    "    * ML 모델 적용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬 기반의 NLP, Text Mining 패키지 \n",
    "\n",
    "사이킷런으로도 일정 수준의 텍스트 분석 기능을 제공하고 있으나, 일반적으로 아래와 같은 전용 패키지와 결합하여 사용한다. \n",
    "\n",
    "* NLTK \n",
    "    * 파이썬의 가장 대표적인 NLP 패키지. \n",
    "    * 방대한 Dataset 와 서브 모듈을 가지고 있음.\n",
    "    * 수행 성능과 정확도, 신기술 기능 지원 등에서 부족한 면을 보임.\n",
    "  \n",
    "  \n",
    "* Gensim \n",
    "    * Topic modeling 에서 가장 두각을 드러내는 패키지\n",
    "    * Word2Vec 구현 등의 다양한 신기능 지원 \n",
    "    * 가장 많이 사용\n",
    "    \n",
    "    \n",
    "* SpaCy \n",
    "    * 뛰어난 성능으로 최근 가장 주목 받는 패키지 \n",
    "    * Gensim 과 함께 가장 많이 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 사전 준비 작업 (텍스트 전처리) - 텍스트 정규화 \n",
    "\n",
    "#### 텍스트 정규화 작업 종류 \n",
    "* 클렌징 Cleansing \n",
    "* 토큰화 Tokenization \n",
    "* 필터링 = Stop word 제거 / 철자 수정 \n",
    "* Stemming\n",
    "* Lemmatization \n",
    "\n",
    "### Cleansing\n",
    "\n",
    "텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업 <br>\n",
    "예를 들어 HTML, XML 태그나 특정 기호 등을 사전에 제거 \n",
    "\n",
    "### Tokenization\n",
    "\n",
    "* 문장 토큰화\n",
    "    * 문서에서 문장을 토큰으로 분리하는 것  \n",
    "    * 문장의 마침표 (.) , 개행문자 (\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리 \n",
    "\n",
    "\n",
    "* 단어 토큰화\n",
    "    * 문장에서 단어를 토큰으로 분리하는 것 \n",
    "    * 일반적으로 공백, 콤마(,) 에 따라 분리 \n",
    "    * 상황에 따라 BOW 같이 단어 순서가 중요하지 않은 경우, 마침표(.), 개행문자(\\n) 등으로 단어 분리도 가능  \n",
    "\n",
    "\n",
    "둘 다 정규 표현식에 따른 토큰화 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 마침표 , 개행 문자 등의 데이터 세트 다운로드 \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3 \n",
      "\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# 문장 토큰화 #\n",
    "###############\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                You can see it out your window or on your television. \\\n",
    "                You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "# sent_tokenize 함수는 list 객체를 반환 \n",
    "print(type(sentences),len(sentences),'\\n')\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "# 만약 마침표가 없었다면 1문장으로 인식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15 \n",
      "\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# 단어 토큰화 #\n",
    "###############\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# 마찬가지로 list 객체로 반환 \n",
    "print(type(words),len(words),'\\n')\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3 \n",
      "\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# 텍스트에 대한 문장 별 단어 토큰화 #\n",
    "#####################################\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 문장을 단어 별로 분리 \n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "print(type(word_tokens), len(word_tokens),'\\n')\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word\n",
    "\n",
    "스톱 워드는 분석에 큰 의미가 없는 단어를 지칭한다. <br><br>\n",
    "예를 들어 영어에서 is , the , a 와 같이 문장을 구성하는 필수 요소이지만 문맥적으로는 의미가 크게 없는 단어들이 이에 해당한다.<br>\n",
    "이러한 단어들은 문장 구성을 위해 빈번하게 등장 할 수 밖에 없는데, 이 것을 중요한 단어로 인지 할 수 있기 때문에 사전 제거는 필수이다. <br><br>\n",
    "NLTK 는 다양한 언어별로 stopwords 를 제공한다. 어떤 것이 있는지 확인해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수:  179 \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수: ',len(nltk.corpus.stopwords.words('english')),'\\n')\n",
    "print(nltk.corpus.stopwords.words('english')[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 stopwords 에 어떤 단어들이 속해 있는지 확인했다.<br><br>\n",
    "이제는 위에서 3개의 문장에 각 각 단어 토큰화를 적용하여 분리시켰던 word_tokens list 중 에서 <br>stopwords 인 것들을 전부 제거해서 의미 있는 단어만 추출 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# sentence = 단어로 분리되어 있는 각 문장 \n",
    "for sentence in word_tokens:\n",
    "    \n",
    "    # 의미 있는 단어를 넣어둘 리스트\n",
    "    filtered_words = []\n",
    "    \n",
    "    # word = 해당 문자을 구성하는 분리된 각 단어들\n",
    "    for word in sentence:\n",
    "        \n",
    "        # 소문자로 변환 - stopwords 에는 기본적으로 위에서 확인 한 바와 같이 소문자로 저장되어 있음\n",
    "        word = word.lower()\n",
    "        \n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filtered_words)        \n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 is , this , all 등이 필터링 된 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming 과 Lemmatization \n",
    "\n",
    "많은 언어에서 문법적인 요소에 따라 단어가 다양하게 변한다. <br>\n",
    "예를 들어, work 라는 동사원형은 worked(과거형) , working(진행형) , works(3인칭 단수) 등 다양한 형태를 가질 수 있는데,<br>\n",
    "Stemming 과 Lemmatization 은 이처럼 __문법적, 의미적으로 변한 단어의 원형을 찾는 기법__ 이다.\n",
    "* <u>Stemming</u><br><br>\n",
    "    * 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음<br><br>\n",
    "    * NLTK 에서 제공하는 Stemmer 종류\n",
    "        * Porter\n",
    "        * Lancaster\n",
    "        * Snowball Stemmer\n",
    "    \n",
    "    \n",
    "* <u>Lemmatization</u><br><br>\n",
    "    * 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아줌\n",
    "    * Stemming 보다 변환 시간이 김<br><br>\n",
    "    * NLTK 에서 제공하는 Lemmatizer 종류\n",
    "        * WordNetLemmatizer      \n",
    "        \n",
    "<br>     \n",
    "Lancaster Stemmer 을 먼저 사용해서 다양한 형태의 단어를 원형으로 변환해 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원형 : work /// work work work\n",
      "원형 : amuse /// amus amus amus\n",
      "원형 : happy /// happy happiest\n",
      "원형 : fancy /// fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print('원형 : work ///',stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print('원형 : amuse ///',stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print('원형 : happy ///',stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print('원형 : fancy ///',stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면\n",
    "* work 는 세 가지 형태가 모두 원형으로 정상 변환\n",
    "* amuse 는 세 가지 형태 모두 원형의 e 누락 \n",
    "* happy 는 iest 문법 변환 오류 \n",
    "* fancy 는 ier, iest 문법 변환 오류 및 원형 누락 \n",
    "\n",
    "등 완벽하게 원형을 찾아내지 못한 것을 볼 수 있다.<br><br>\n",
    "이번에는 Lemmatizer 의 WordNetLemmatizer 을 이용해서 다양한 형태의 단어를 원형으로 변환해 보자. <br><br>\n",
    "일반적으로 Lemmatization 은 보다 정확한 원형 단어 추출을 위해서 단어의 '품사' 까지 입력 해주어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원형 : amuse /// amuse amuse amuse\n",
      "원형 : happy /// happy happy\n",
      "원형 : fancy /// fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print('원형 : amuse ///',lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print('원형 : happy ///',lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print('원형 : fancy ///',lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmer 보다 더 정확하게 동사원형을 찾아 낸 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 피처 벡터화 / 피처 추출 \n",
    "\n",
    "### BOW - Bag of Words\n",
    "\n",
    "[BOW 모델](https://www.programmersought.com/images/947/0acb9279d17a1631bcfb154583cca443.JPEG) 은 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델이다. <br><br>\n",
    "다음 [예시](https://miro.medium.com/max/1536/1*3IACMnNpwVlCl8kSTJocPA.png) 를 보자. ( p.476, 477 참고 )\n",
    "* 수행 순서\n",
    "    1. 모든 document 에 있는 단어 전부에서 중복을 제거하고 각 문장을 구성하고 있는 단어들을 피처로 나열한다.\n",
    "    2. 각 단어에 고유의 인덱스 값을 부여한다. ex)  the:0 , cat:1 , sat:2 ... ( 원래는 stopwords 제거 한 뒤에 피처 추출을 수행한다 ) \n",
    "    3. 해당 document 를 구성하는 각 단어의 갯수 = 빈도 값 을 피처의 데이터 값으로 기재한다. \n",
    "    \n",
    "    \n",
    "* 특징\n",
    "    * Dataframe , Matrix 형태를 띈다.\n",
    "    * Row 에는 document &nbsp;/&nbsp; Columns 에는 document 를 구성하는 단어   \n",
    "<br><br>\n",
    "\n",
    "이처럼 BOW 모델은 쉽고 빠른 구축을 할 수 있다. 단순히 단어의 발생 횟수에 기반하고 있음에도 문서의 특징을 잘 나타 낼 수 있어서 활용도가 높다.<br><br>\n",
    "하지만 그만큼 단점도 분명하다.\n",
    "* __문맥 의미 반영 부족__<br><br>\n",
    "    * BOW 는 단어의 순서를 고려 하지 않기 때문에 문맥적인 의미가 무시 된다. 이를 보완하기 위해 n_gram 기법을 사용하지만 제한적이다.<br><br>   \n",
    "* __희소 행렬 문제__<br><br>\n",
    "    * 희소 행렬 : 행렬의 대부분의 값이 0인 행렬<br><br>\n",
    "    * 일반적으로 여러 텍스트에서 stopword 를 제외하고 같은 단어가 많이 사용되기는 어렵다. <br>\n",
    "      그래서 BOW 로 많은 문서에 대해 피처 벡터화를 진행하면, 수 많은 피처가 만들어 질 것이고 대부분은 0 으로 채워 질 수 밖에 없다.<br>\n",
    "      이처럼 희소 행렬 형태를 띄게 되는데, 희소 행렬은 ML 알고리즘의 수행 시간과 예측 성능을 떨어뜨리기 때문에 특별한 기법을 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW 의 피처 벡터화의 두 가지 방식 \n",
    "<br>\n",
    "\n",
    "__< Count 기반의 벡터화 >__\n",
    "* <u>정의</u>\n",
    "    * 위와 같이 단어 피처에 값을 부여할 때, 각 문서에서 해당 단어가 나타나는 횟수 = count 를 부여하는 방식\n",
    "\n",
    "\n",
    "* count 값이 높을수록 중요한 단어로 인식 \n",
    "\n",
    "\n",
    "* <u>한계</u>\n",
    "    * 중요하지 않고 문맥상 자주 사용 되는 단어까지 높은 값을 부여 받게 된다. 이 문제로 인해 TF-IDF 등장\n",
    "    \n",
    "    \n",
    "* Scikit-learn 에서 구현 \n",
    "    * CountVectorizer 클래스는 소문자 일괄 변환, tokenization, stop word 등 텍스트 전처리도 함께 수행한다. <br>\n",
    "    단, Stemmer 와 Lammatize 는 지원을 하지 않지 때문에 함수를 만드는 편이 좋다. <br><br>\n",
    "    * [CountVectorizer 공식문서](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countve#sklearn.feature_extraction.text.CountVectorizer) ⇒ 이해가 어려우면 p.480 참고 \n",
    "<br><br>\n",
    "\n",
    "__< TF-IDF 기반의 벡터화 >__\n",
    "* <u>정의</u> \n",
    "    * 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 주는 방식<br>\n",
    "    * ex) '많은' , '당연히' , '업무' 등이 문서 주제와 관련성이 적지만 보편적으로 많이 사용되는 단어 \n",
    "\n",
    "\n",
    "* TF-IDF [공식](https://class101.dev/images/thumbnails/tf-idf.png) \n",
    "    * W = tf x log ( N / df ) \n",
    "    * 가중치 = 개별 문서에서의 단어 i 빈도 x log ( 전체 문서 수 / 단어 i 를 가지고 있는 문서 수 ) \n",
    "    * <span style=\"color:red\">즉, tf 에 log 을 곱해서 페널티를 부여한 W 를 가중치로 갖는 방식 </span>\n",
    "    \n",
    "    \n",
    "* Scikit-learn 에서 구현 \n",
    "    * [TfidfVectorizer 공식문서](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW 벡터화를 위한 희소 행렬 \n",
    "\n",
    "앞서 BOW 모델 소개에서 피처 벡터화 결과물이 왜 희소 행렬이고, 이 행렬이 의미하는 바를 간략하게 알아봤다.<br><br>\n",
    "희소 행렬은 불필요한 0 값이 메모리 공간에 할당되어 메모리 공간을 쓸데 없이 많이 잡아 먹기 때문에 시간도 많이 걸리고 비효율적이다.<br><br>\n",
    "그래서 이런 희소 행렬을 물리적으로 적은 메모리 공간을 차지 할 수 있도록 변환하기 위한 방법으로 \n",
    "* COO 형식<br>\n",
    "* CSR 형식\n",
    "\n",
    "이 있다.\n",
    "\n",
    "#### < 희소 행렬 - COO 형식 > \n",
    "\n",
    "* COO 형식은 0 이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가르키는 행과 열의 위치를 별도의 배열로 저장하는 방식 \n",
    "\n",
    "[ Example ] <br>\n",
    "\n",
    "||단어 1|단어 2|단어 3|\n",
    "|------|---|---|---|\n",
    "|document 1|3|0|1|\n",
    "|document 2|0|2|0|\n",
    "|document 3|1|1|0|\n",
    "\n",
    "위와 같은 2차원 데이터가 있다고 가정하자. <br><br>\n",
    "0 이 아닌 데이터는 [ 3 , 1 , 2 , 1 , 1 ] 이고, 0 이 아닌 데이터가 있는 위치를 ( row,col ) 로 표시하면 (0,0) (0,2) (1,1) (2,0) (2,1) 가 된다. <br><br>\n",
    "row 와 col 을 별도의 배열로 저장하면 __row = [ 0 , 0 , 1 , 2 , 2 ] , col = [ 0 , 2 , 1 , 0 , 1 ]__ <br><br>\n",
    "이런 희소 행렬 변환은 파이썬에서 Scipy 를 이용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "희소 행렬 데이터 값:  [3 1 2 1 1]\n",
      "희소 행렬 각 데이터의 col 인덱스:  [0 2 1 0 1]\n",
      "희소 행렬 각 데이터의 row 인덱스:  [0 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "dense = np.array([[3,0,1],[0,2,0],[1,1,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2,1,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각 각 배열로 생성 \n",
    "row_index = np.array([0,0,1,2,2])\n",
    "col_index = np.array([0,2,1,0,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix 를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data,(row_index,col_index)))\n",
    "\n",
    "# COO 형식의 희소 행렬 객체 변수 \n",
    "sparse_coo\n",
    "\n",
    "# 활용 \n",
    "print(\"희소 행렬 데이터 값: \", sparse_coo.data)\n",
    "print(\"희소 행렬 각 데이터의 col 인덱스: \",sparse_coo.col)\n",
    "print(\"희소 행렬 각 데이터의 row 인덱스: \",sparse_coo.row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 원래 데이터 행렬로 \n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### < 희소 행렬 - CSR 형식 >\n",
    "\n",
    "* CSR 형식은 COO 형식의 row 와 column 의 위치 인덱스를 나타내는 배열에서 반복적인 데이터를 사용해야 하는 문제점을 해결한 방식\n",
    "\n",
    "먼저 COO 형식의 문제점을 알아보자. <br><br>\n",
    "COO 형식의 경우 위 결과 값처럼 희소 행렬을 구성하는 0 이 아닌 데이터가 많으면 많을수록 col, row 인덱스를 저장하는 배열의 길이가 길어진다.<br><br>\n",
    "특히, row 인덱스의 경우 앞 예제에서 [0, 0, 1, 2, 2] 처럼 0 부터 순차적으로 증가하는 값으로 이뤄졌다는 특성을 고려하면 <br><br>\n",
    "<span style=\"color:red\">row 인덱스의 배열 내에 있는 고유한 값이 변하기 시작하는 시작 인덱스를 별도의 배열로 표기하여 반복을 제거 할 수 있다. </span><br><br>\n",
    "간단한 예로, \n",
    "> row 인덱스 배열 : [0,0,1,1,1,1,1,2,2,2,2,3,4,4,5,5] 이라 가정 <br><br>\n",
    "> 고유한 값이 변하기 시작하는 시점의 인덱스 배열 : [0,2,7,11,12,14] \n",
    "\n",
    "이제 새롭게 만들어진 배열의 맨 마지막에는 데이터의 총 항목 개수를 추가 해주어야 한다.\n",
    "> 데이터의 총 항목 개수를 추가한 최종 배열 : [0,2,7,11,12,14,16]\n",
    "\n",
    "이런 변환 방식을 CSR 이라 한다. \n",
    "\n",
    "<br><br>\n",
    "CSR 방식의 변환은 Scipy 의 csr_matrix 클래스를 이용해 쉽게 구할 수 있다. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 확인하기 위해 변환된 것 다시 array 로 \n",
      "\n",
      " [[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]] \n",
      "\n",
      "\n",
      "CSR 변환된 데이터가 제대로 되었는지 확인하기 위해 변환된 것 다시 array 로 \n",
      "\n",
      " [[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[0,0,1,0,0,5],\n",
    "                 [1,4,0,3,2,5],\n",
    "                 [0,6,0,3,0,0],\n",
    "                 [2,0,0,0,0,0],\n",
    "                 [0,0,0,7,0,8],\n",
    "                 [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# col 인덱스 배열과 row 인덱스 배열 추출 \n",
    "col_index = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "row_index = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data,(row_index,col_index)))\n",
    "\n",
    "# row 인덱스 배열에서 고유한 값의 각 시작 인덱스를 배열로 생성 \n",
    "row_index_index = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data,col_index,row_index_index))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 확인하기 위해 변환된 것 다시 array 로 \\n\\n',sparse_coo.toarray(),'\\n\\n')\n",
    "\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 확인하기 위해 변환된 것 다시 array 로 \\n\\n',sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 COO , CSR 두 형식으로 변환 되는 원리와 과정을 알기 위해 직접 각 과정 array 를 생각해여 구성 시킨 것을 coo_matrix , csr_matrix <br> \n",
    "에 넣어 주었지만, 실제로는 아래와 같이 희소 행렬 자체를 coo_matrix , csr_matrix 함수의 파라미터로 넣어주면 앞서 배운 이론에 맞게 자동 변환 해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### COO 형식으로 변환 ###\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1 \n",
      "\n",
      "### CSR 형식으로 변환 ###\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "test_arr = np.array([[0,0,1,0,0,5],\n",
    "                 [1,4,0,3,2,5],\n",
    "                 [0,6,0,3,0,0],\n",
    "                 [2,0,0,0,0,0],\n",
    "                 [0,0,0,7,0,8],\n",
    "                 [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(test_arr)\n",
    "csr = sparse.csr_matrix(test_arr)\n",
    "\n",
    "print('### COO 형식으로 변환 ###')\n",
    "print(coo,'\\n')\n",
    "\n",
    "print('### CSR 형식으로 변환 ###')\n",
    "print(csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "[질문] <br>\n",
    "여기에서 CSR 형식 변환은 row 인덱스 배열에서 고유값의 변화가 일어나는 시작 인덱스들로 구성한 배열이 결과값으로 안나오던데, 이러면 두 형식의 함수를 나눠 둔 이유가 있을까? CSR 에서 반복되는 인덱스 제거 처리를 못한거 같은데 이러면 coo_matrix 함수랑 다른게 뭐가 있는거지 ?  <br><br>\n",
    "[SciPy 공식문서 csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) 를 찾아봐도 반환 값이 전혀 이해가 가지 않는다. <br><br> 반환 값은 같고 실제 메모리 사용량에서 차이가 있다고만 이해 하고 넘어가자. \n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 COO 와 CSR 이 어떻게 희소 행렬의 메모리를 줄이는지 알아 보았다. <br><br>\n",
    "다시 BOW 로 돌아와서, <br><br>\n",
    "사이킷런의 CountVectorizer 나 TfidfVectorizer 클래스로 변환된 피처 벡터화 형렬은 모두 SciPy 의 CSR 형태의 희소 행렬이다. <br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
